{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Colab Setup for netFound Pre-processing (rev-2, 2025-05-28) ---\n",
        "\"\"\"\n",
        "Single-cell script that prepares a Google Colab runtime for the **netFound**\n",
        "pre-processing pipeline.\n",
        "\n",
        "Key points\n",
        "-----------\n",
        "* **PcapPlusPlus** ‚Äì builds from *master* by default (change `PCAPPP_TAG` to a\n",
        "  release tag like `v24.09` for a reproducible build).  A zip-archive fallback\n",
        "  is used if *git clone* fails (GitHub error 128, network hiccup, etc.).\n",
        "* **No duplicated blocks** ‚Äì previous edits accidentally left two build paths;\n",
        "  this revision consolidates them.\n",
        "* **Safe temp cleanup** ‚Äì avoids `FileNotFoundError` if the unzip directory is\n",
        "  moved.\n",
        "* **Helper utilities** ‚Äì `run()` prints every command; `ensure_dir()` creates\n",
        "  paths idempotently.\n",
        "\"\"\"\n",
        "\n",
        "import io\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Helper utilities\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def run(cmd, **kw):\n",
        "    \"\"\"Wrapper around subprocess.run that logs and checks exit status.\"\"\"\n",
        "    print(\"‚Üí\", \" \".join(map(str, cmd)))\n",
        "    subprocess.run(cmd, check=True, text=True, **kw)\n",
        "\n",
        "\n",
        "def ensure_dir(path: Path):\n",
        "    if not path.exists():\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Constants (edit here if needed)\n",
        "# ---------------------------------------------------------------------------\n",
        "NETFOUND_ZIP_PATH = Path(\"/content/netFound.zip\")\n",
        "NETFOUND_DIR      = Path(\"/content/netFound\")\n",
        "\n",
        "PCAPPP_TAG  = \"master\"   # use a tag (e.g. \"v24.09\") for deterministic builds\n",
        "PCAPPP_REPO = \"https://github.com/seladb/PcapPlusPlus.git\"\n",
        "PCAPPP_DIR  = Path(\"/content/PcapPlusPlus\")\n",
        "\n",
        "PCAPSPLITTER_REPO = \"https://github.com/shramos/pcap-splitter.git\"\n",
        "PCAPSPLITTER_DIR  = Path(\"/content/pcap-splitter\")\n",
        "PCAPSPLITTER_BIN  = Path(\"/usr/local/bin/PcapSplitter\")\n",
        "\n",
        "APT_DEPS = [\"cmake\", \"g++\", \"libpcap-dev\", \"git\", \"parallel\", \"make\"]\n",
        "PY_REQS_FALLBACK = [\n",
        "    \"pyarrow\", \"pandas\", \"datasets\", \"transformers\",\n",
        "    \"scikit-learn\", \"huggingface_hub\", \"torch\", \"google-generativeai\",\n",
        "]\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 0 ‚Äì Ensure the user uploaded netFound.zip\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n=== 0 / 6  Check for netFound.zip ===\")\n",
        "if not NETFOUND_ZIP_PATH.exists():\n",
        "    print(\"[USER ACTION]  Please upload 'netFound.zip' to /content and rerun.\")\n",
        "    sys.exit(1)\n",
        "print(\"Found\", NETFOUND_ZIP_PATH)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 1 ‚Äì Unzip netFound repository\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n=== 1 / 6  Unzipping netFound ===\")\n",
        "if NETFOUND_DIR.exists():\n",
        "    shutil.rmtree(NETFOUND_DIR)\n",
        "\n",
        "temp_dir = Path(\"/content/_nf_unzip\")\n",
        "if temp_dir.exists():\n",
        "    shutil.rmtree(temp_dir)\n",
        "ensure_dir(temp_dir)\n",
        "run([\"unzip\", \"-qq\", str(NETFOUND_ZIP_PATH), \"-d\", str(temp_dir)])\n",
        "\n",
        "items = list(temp_dir.iterdir())\n",
        "repo_root = items[0] if len(items) == 1 else temp_dir\n",
        "shutil.move(str(repo_root), str(NETFOUND_DIR))\n",
        "if temp_dir.exists():\n",
        "    shutil.rmtree(temp_dir)\n",
        "print(\"Unzipped into\", NETFOUND_DIR)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 2 ‚Äì Install system dependencies\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n=== 2 / 6  Installing apt dependencies ===\")\n",
        "run([\"apt-get\", \"update\", \"-qq\"])\n",
        "run([\"apt-get\", \"install\", \"-y\", \"-qq\", *APT_DEPS])\n",
        "print(\"System packages installed.\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3 ‚Äì Clone + build PcapPlusPlus (with zip fallback)\n",
        "# ---------------------------------------------------------------------------\n",
        "print(f\"\\n=== 3 / 6  Building PcapPlusPlus ({PCAPPP_TAG}) ===\")\n",
        "if PCAPPP_DIR.exists():\n",
        "    shutil.rmtree(PCAPPP_DIR)\n",
        "\n",
        "\n",
        "def download_pcappp_zip(ref: str):\n",
        "    \"\"\"Download and unpack a zip archive for the given branch or tag.\"\"\"\n",
        "    if ref == \"master\":\n",
        "        url = f\"https://github.com/seladb/PcapPlusPlus/archive/refs/heads/{ref}.zip\"\n",
        "    else:\n",
        "        url = f\"https://github.com/seladb/PcapPlusPlus/archive/refs/tags/{ref}.zip\"\n",
        "    print(\"[fallback] downloading\", url)\n",
        "    with urllib.request.urlopen(url) as rsp:\n",
        "        data = rsp.read()\n",
        "    with zipfile.ZipFile(io.BytesIO(data)) as zf:\n",
        "        zf.extractall(PCAPPP_DIR.parent)\n",
        "    extracted = PCAPPP_DIR.parent / f\"PcapPlusPlus-{ref}\"\n",
        "    extracted.rename(PCAPPP_DIR)\n",
        "\n",
        "try:\n",
        "    if PCAPPP_TAG == \"master\":\n",
        "        run([\"git\", \"clone\", \"--depth\", \"1\", PCAPPP_REPO, str(PCAPPP_DIR)])\n",
        "    else:\n",
        "        run([\"git\", \"clone\", \"--depth\", \"1\", \"--branch\", PCAPPP_TAG,\n",
        "             PCAPPP_REPO, str(PCAPPP_DIR)])\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"[WARN] git clone failed (exit {e.returncode}); falling back to zip ‚Ä¶\")\n",
        "    download_pcappp_zip(PCAPPP_TAG)\n",
        "\n",
        "# Build with CMake\n",
        "os.chdir(PCAPPP_DIR)\n",
        "run([\"cmake\", \"-S\", \".\", \"-B\", \"build\", \"-DCMAKE_BUILD_TYPE=Release\"])\n",
        "run([\"cmake\", \"--build\", \"build\", \"-j\", \"2\"])\n",
        "run([\"sudo\", \"cmake\", \"--install\", \"build\"])\n",
        "print(\"PcapPlusPlus installed in /usr/local\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4 ‚Äì Build + install PcapSplitter\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n=== 4 / 6  Building PcapSplitter ===\")\n",
        "if not PCAPSPLITTER_BIN.exists():\n",
        "    if PCAPSPLITTER_DIR.exists():\n",
        "        shutil.rmtree(PCAPSPLITTER_DIR)\n",
        "    run([\"git\", \"clone\", PCAPSPLITTER_REPO, str(PCAPSPLITTER_DIR)])\n",
        "    os.chdir(PCAPSPLITTER_DIR)\n",
        "    run([\"make\", \"-j\", \"2\"])\n",
        "    run([\"sudo\", \"cp\", \"pcapsplitter\", str(PCAPSPLITTER_BIN)])\n",
        "    run([\"sudo\", \"chmod\", \"+x\", str(PCAPSPLITTER_BIN)])\n",
        "    print(\"PcapSplitter installed.\")\n",
        "else:\n",
        "    print(\"PcapSplitter already present ‚Äì skipping build.\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5 ‚Äì Compile netFound C++ helpers\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n=== 5 / 6  Building netFound helpers ===\")\n",
        "cpp_src   = NETFOUND_DIR / \"src/pre_process/packets_processing_src\"\n",
        "cpp_build = cpp_src / \"build\"\n",
        "ensure_dir(cpp_build)\n",
        "\n",
        "os.chdir(cpp_build)\n",
        "if not (cpp_src / \"CMakeLists.txt\").exists():\n",
        "    print(\"[WARN] CMakeLists.txt missing ‚Äì helpers skipped.\")\n",
        "else:\n",
        "    run([\"cmake\", \"..\"])\n",
        "    run([\"make\", \"-j\", \"2\"])\n",
        "    helpers_out = NETFOUND_DIR / \"src/pre_process\"\n",
        "    for tool in (\"1_filter\", \"3_field_extraction\"):\n",
        "        built = cpp_build / tool\n",
        "        if built.exists():\n",
        "            shutil.copy2(built, helpers_out / tool)\n",
        "            (helpers_out / tool).chmod(0o755)\n",
        "            print(\"  ‚Ä¢\", tool, \"built ‚Üí\", helpers_out / tool)\n",
        "        else:\n",
        "            print(\"  ‚Ä¢ [WARN]\", tool, \"missing in build output\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 6 ‚Äì Install Python dependencies\n",
        "# ---------------------------------------------------------------------------\n",
        "print(\"\\n=== 6 / 6  Installing Python deps ===\")\n",
        "req = NETFOUND_DIR / \"requirements.txt\"\n",
        "if req.exists():\n",
        "    run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(req)])\n",
        "else:\n",
        "    run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *PY_REQS_FALLBACK])\n",
        "print(\"Python packages installed.\")\n",
        "\n",
        "print(\"\\n=== Setup complete!  Current WD:\", os.getcwd(), \"===\")\n",
        "print(\"Upload PCAPs to 'attack_data/raw/{0,1}' and continue with preprocessing.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 0 / 6  Check for netFound.zip ===\n",
            "Found /content/netFound.zip\n",
            "\n",
            "=== 1 / 6  Unzipping netFound ===\n",
            "‚Üí unzip -qq /content/netFound.zip -d /content/_nf_unzip\n",
            "Unzipped into /content/netFound\n",
            "\n",
            "=== 2 / 6  Installing apt dependencies ===\n",
            "‚Üí apt-get update -qq\n",
            "‚Üí apt-get install -y -qq cmake g++ libpcap-dev git parallel make\n",
            "System packages installed.\n",
            "\n",
            "=== 3 / 6  Building PcapPlusPlus (master) ===\n",
            "‚Üí git clone --depth 1 https://github.com/seladb/PcapPlusPlus.git /content/PcapPlusPlus\n",
            "‚Üí cmake -S . -B build -DCMAKE_BUILD_TYPE=Release\n",
            "‚Üí cmake --build build -j 2\n",
            "‚Üí sudo cmake --install build\n",
            "PcapPlusPlus installed in /usr/local\n",
            "\n",
            "=== 4 / 6  Building PcapSplitter ===\n",
            "PcapSplitter already present ‚Äì skipping build.\n",
            "\n",
            "=== 5 / 6  Building netFound helpers ===\n",
            "[WARN] CMakeLists.txt missing ‚Äì helpers skipped.\n",
            "\n",
            "=== 6 / 6  Installing Python deps ===\n",
            "‚Üí /usr/bin/python3 -m pip install -q pyarrow pandas datasets transformers scikit-learn huggingface_hub torch google-generativeai\n",
            "Python packages installed.\n",
            "\n",
            "=== Setup complete!  Current WD: /content/netFound/src/pre_process/packets_processing_src/build ===\n",
            "Upload PCAPs to 'attack_data/raw/{0,1}' and continue with preprocessing.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9gyb_ZK2WvN",
        "outputId": "28375f41-f6cb-4cc1-8ad1-3490642efdb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 2 : Upload netFound scripts, PCAP data & configs (generic file-names) ---\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"üîÑ  User action: upload your data & scripts to /content\")\n",
        "print(\"   ‚Ä¢  Any *.pcap file name is accepted ‚Äì no specific pattern required.\\n\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 1. Ensure directory layout exists (create empty dirs if not uploaded yet)\n",
        "# --------------------------------------------------------------------\n",
        "ATTACK_DIR   = \"/content/attack_data/raw/0\"\n",
        "BENIGN_DIR   = \"/content/attack_data/raw/1\"\n",
        "NF_SRC_DIR   = \"/content/netFound/src/pre_process\"\n",
        "NF_CFG_DIR   = \"/content/netFound/configs\"\n",
        "\n",
        "for d in (ATTACK_DIR, BENIGN_DIR, NF_SRC_DIR, NF_CFG_DIR):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2. Quick inventory of uploaded PCAPs\n",
        "# --------------------------------------------------------------------\n",
        "def count_pcaps(path):\n",
        "    return len(glob.glob(os.path.join(path, \"*.pcap\")) +\n",
        "               glob.glob(os.path.join(path, \"*.PCAP\")))\n",
        "\n",
        "print(f\"üìÇ {ATTACK_DIR}  ‚Üí  {count_pcaps(ATTACK_DIR)} PCAP files detected\")\n",
        "print(f\"üìÇ {BENIGN_DIR}  ‚Üí  {count_pcaps(BENIGN_DIR)} PCAP files detected\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 3. Create a dummy tokenizer config if the user hasn‚Äôt provided one\n",
        "# --------------------------------------------------------------------\n",
        "dummy_cfg = os.path.join(NF_CFG_DIR, \"TestFinetuningConfig.json\")\n",
        "if not os.path.exists(dummy_cfg):\n",
        "    with open(dummy_cfg, \"w\") as fh:\n",
        "        fh.write(\n",
        "            '{\"IPFields\": [], \"TCPFields\": [], \"UDPFields\": [], '\n",
        "            '\"ICMPFields\": [], \"Payload\": [], \"internalIPs\": [\"127.0.0.1/8\"]}'\n",
        "        )\n",
        "    print(f\"üìù  Created placeholder tokenizer config at {dummy_cfg}\")\n",
        "\n",
        "print(\"\\n‚úÖ  Directory structure ready. Proceed to the next cell to compile the C++ tools.\")\n"
      ],
      "metadata": {
        "id": "tCIWfz7t3WeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3e5e5e-40ef-4de9-8e2a-02a7d6b8511e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ  User action: upload your data & scripts to /content\n",
            "   ‚Ä¢  Any *.pcap file name is accepted ‚Äì no specific pattern required.\n",
            "\n",
            "üìÇ /content/attack_data/raw/0  ‚Üí  10 PCAP files detected\n",
            "üìÇ /content/attack_data/raw/1  ‚Üí  10 PCAP files detected\n",
            "üìù  Created placeholder tokenizer config at /content/netFound/configs/TestFinetuningConfig.json\n",
            "\n",
            "‚úÖ  Directory structure ready. Proceed to the next cell to compile the C++ tools.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Cell 3: Compile netFound's C++ Preprocessing Tools ---\n",
        "# This cell assumes you have uploaded the netFound/src/pre_process/packets_processing_src/ directory.\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "NETFOUND_DIR_COLAB = \"/content/netFound/netFound\"\n",
        "CPP_SRC_DIR_COLAB = os.path.join(NETFOUND_DIR_COLAB, \"src/pre_process/packets_processing_src\")\n",
        "CPP_BUILD_DIR_COLAB = os.path.join(CPP_SRC_DIR_COLAB, \"build\")\n",
        "PRE_PROCESS_SCRIPTS_DIR_COLAB = os.path.join(NETFOUND_DIR_COLAB, \"src/pre_process\")\n",
        "\n",
        "if not os.path.exists(CPP_SRC_DIR_COLAB):\n",
        "    print(f\"ERROR: Directory '{CPP_SRC_DIR_COLAB}' not found. Please upload it from your netFound clone.\")\n",
        "else:\n",
        "    print(f\"Compiling netFound C++ tools in {CPP_BUILD_DIR_COLAB}...\")\n",
        "    os.makedirs(CPP_BUILD_DIR_COLAB, exist_ok=True)\n",
        "\n",
        "    original_cwd = os.getcwd()\n",
        "    os.chdir(CPP_BUILD_DIR_COLAB)\n",
        "\n",
        "    cmake_success = False\n",
        "    try:\n",
        "        # Ensure CMakeLists.txt is present\n",
        "        if not os.path.exists(\"../CMakeLists.txt\"):\n",
        "             print(f\"ERROR: CMakeLists.txt not found in {CPP_SRC_DIR_COLAB}. Cannot compile C++ tools.\")\n",
        "        else:\n",
        "            subprocess.run([\"cmake\", \"..\"], check=True, capture_output=True)\n",
        "            subprocess.run([\"make\", \"-j\", \"2\", \"-s\"], check=True) # -s for silent\n",
        "            cmake_success = True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"ERROR during C++ tools compilation (cmake or make): {e}\")\n",
        "        print(\"STDOUT:\", e.stdout.decode() if e.stdout else \"N/A\")\n",
        "        print(\"STDERR:\", e.stderr.decode() if e.stderr else \"N/A\")\n",
        "    finally:\n",
        "        os.chdir(original_cwd)\n",
        "\n",
        "    if cmake_success:\n",
        "        print(\"Copying compiled tools to src/pre_process/...\")\n",
        "        for tool_name in [\"1_filter\", \"3_field_extraction\"]:\n",
        "            source_path = os.path.join(CPP_BUILD_DIR_COLAB, tool_name)\n",
        "            dest_path = os.path.join(PRE_PROCESS_SCRIPTS_DIR_COLAB, tool_name)\n",
        "            if os.path.exists(source_path):\n",
        "                subprocess.run([\"cp\", source_path, dest_path], check=True)\n",
        "                subprocess.run([\"chmod\", \"+x\", dest_path], check=True)\n",
        "                print(f\"  Copied and set +x for {dest_path}\")\n",
        "            else:\n",
        "                print(f\"  ERROR: Compiled tool '{tool_name}' not found at {source_path}.\")\n",
        "        print(\"netFound C++ tools compiled and copied.\")\n",
        "    else:\n",
        "        print(\"C++ tools compilation failed. Preprocessing scripts may not run correctly.\")\n",
        "\n",
        "print(\"\\nSetup for C++ tools complete. Proceed to run the preprocessing pipeline.\")\n"
      ],
      "metadata": {
        "id": "MPDGTLjj3h-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abdc0f6e-29ec-410e-83c4-fa6e1c5b5fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling netFound C++ tools in /content/netFound/netFound/src/pre_process/packets_processing_src/build...\n",
            "Copying compiled tools to src/pre_process/...\n",
            "  Copied and set +x for /content/netFound/netFound/src/pre_process/1_filter\n",
            "  Copied and set +x for /content/netFound/netFound/src/pre_process/3_field_extraction\n",
            "netFound C++ tools compiled and copied.\n",
            "\n",
            "Setup for C++ tools complete. Proceed to run the preprocessing pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Cell 4: Run Local-Style Preprocessing Script in Colab ---\n",
        "# This cell executes the bash script logic from your \"Part 1\" directly in Colab.\n",
        "# It assumes Cells 1, 2, and 3 have completed successfully.\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"Starting local-style netFound PCAP preprocessing pipeline in Colab...\")\n",
        "\n",
        "# --- Configuration (paths are relative to /content/ or absolute) ---\n",
        "NETFOUND_REPO_DIR_COLAB = \"/content/netFound\"\n",
        "INPUT_DATA_ROOT_FOLDER_COLAB = \"/content/attack_data\" # Contains raw/0/*.pcap and raw/1/*.pcap\n",
        "TOKENIZER_JSON_CONFIG_COLAB = os.path.join(NETFOUND_REPO_DIR_COLAB, \"configs/TestFinetuningConfig.json\")\n",
        "FINAL_ARROW_OUTPUT_BASE_COLAB = os.path.join(INPUT_DATA_ROOT_FOLDER_COLAB, \"final\", \"arrow_per_pcap\")\n",
        "\n",
        "FILTER_SCRIPT_COLAB = os.path.join(NETFOUND_REPO_DIR_COLAB, \"netFound/src/pre_process/1_filter.sh\")\n",
        "SPLIT_SCRIPT_COLAB = os.path.join(NETFOUND_REPO_DIR_COLAB, \"netFound/src/pre_process/2_pcap_splitting.sh\")\n",
        "EXTRACT_SCRIPT_COLAB = os.path.join(NETFOUND_REPO_DIR_COLAB, \"netFound/src/pre_process/3_extract_fields.sh\")\n",
        "TOKENIZE_PY_SCRIPT_COLAB = os.path.join(NETFOUND_REPO_DIR_COLAB, \"netFound/src/pre_process/Tokenize.py\")\n",
        "COLLECT_PY_SCRIPT_COLAB = os.path.join(NETFOUND_REPO_DIR_COLAB, \"netFound/src/pre_process/CollectTokensInFiles.py\")\n",
        "TCP_OPTIONS_FLAG_COLAB = \"0\"\n",
        "\n",
        "# Ensure shell scripts are executable (might have been done in Cell 1, but good to re-check)\n",
        "for script_path in [FILTER_SCRIPT_COLAB, SPLIT_SCRIPT_COLAB, EXTRACT_SCRIPT_COLAB]:\n",
        "    if os.path.exists(script_path):\n",
        "        subprocess.run([\"chmod\", \"+x\", script_path], check=True)\n",
        "    else:\n",
        "        print(f\"Warning: Script {script_path} not found!\")\n",
        "\n",
        "\n",
        "# --- Start Preprocessing ---\n",
        "print(f\"netFound Repository in Colab: {NETFOUND_REPO_DIR_COLAB}\")\n",
        "print(f\"Input Data Root in Colab: {INPUT_DATA_ROOT_FOLDER_COLAB}\")\n",
        "print(f\"Tokenizer Config: {TOKENIZER_JSON_CONFIG_COLAB}\")\n",
        "print(f\"Final Arrow Output Base: {FINAL_ARROW_OUTPUT_BASE_COLAB}\")\n",
        "\n",
        "for label_dir_name_colab in [\"0\", \"1\"]:\n",
        "    print(f\"\\n--- Processing Label: {label_dir_name_colab} ---\")\n",
        "\n",
        "    RAW_PCAP_INPUT_DIR_C = os.path.join(INPUT_DATA_ROOT_FOLDER_COLAB, \"raw\", label_dir_name_colab)\n",
        "    FILTERED_PCAP_DIR_C = os.path.join(INPUT_DATA_ROOT_FOLDER_COLAB, \"filtered\", label_dir_name_colab)\n",
        "    SPLIT_FLOWS_DIR_C = os.path.join(INPUT_DATA_ROOT_FOLDER_COLAB, \"split\", label_dir_name_colab)\n",
        "    EXTRACTED_FIELDS_DIR_C = os.path.join(INPUT_DATA_ROOT_FOLDER_COLAB, \"extracted\", label_dir_name_colab)\n",
        "    FINAL_ARROW_LABEL_DIR_C = os.path.join(FINAL_ARROW_OUTPUT_BASE_COLAB, label_dir_name_colab)\n",
        "\n",
        "    os.makedirs(FILTERED_PCAP_DIR_C, exist_ok=True)\n",
        "    os.makedirs(SPLIT_FLOWS_DIR_C, exist_ok=True)\n",
        "    os.makedirs(EXTRACTED_FIELDS_DIR_C, exist_ok=True)\n",
        "    os.makedirs(FINAL_ARROW_LABEL_DIR_C, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(RAW_PCAP_INPUT_DIR_C) or not any(f.endswith('.pcap') for f in os.listdir(RAW_PCAP_INPUT_DIR_C)):\n",
        "        print(f\"  No PCAP files found in {RAW_PCAP_INPUT_DIR_C}. Skipping label {label_dir_name_colab}.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"  [1/4] Filtering PCAPs from {RAW_PCAP_INPUT_DIR_C} to {FILTERED_PCAP_DIR_C}...\")\n",
        "    cmd_filter = [\"bash\", FILTER_SCRIPT_COLAB, RAW_PCAP_INPUT_DIR_C, FILTERED_PCAP_DIR_C]\n",
        "    result = subprocess.run(cmd_filter, capture_output=True, text=True)\n",
        "    if result.returncode != 0: print(f\"    Error in 1_filter.sh: {result.stderr}\") #else: print(f\"    Filter Output: {result.stdout[:200]}...\")\n",
        "\n",
        "    print(f\"  [2/4] Splitting PCAPs from {FILTERED_PCAP_DIR_C} into flows in {SPLIT_FLOWS_DIR_C}...\")\n",
        "    cmd_split = [\"bash\", SPLIT_SCRIPT_COLAB, FILTERED_PCAP_DIR_C, SPLIT_FLOWS_DIR_C]\n",
        "    result = subprocess.run(cmd_split, capture_output=True, text=True)\n",
        "    if result.returncode != 0: print(f\"    Error in 2_pcap_splitting.sh: {result.stderr}\") #else: print(f\"    Split Output: {result.stdout[:200]}...\")\n",
        "\n",
        "    print(f\"  [3/4] Extracting fields from flows in {SPLIT_FLOWS_DIR_C} to {EXTRACTED_FIELDS_DIR_C}...\")\n",
        "    cmd_extract = [\"bash\", EXTRACT_SCRIPT_COLAB, SPLIT_FLOWS_DIR_C, EXTRACTED_FIELDS_DIR_C, TCP_OPTIONS_FLAG_COLAB]\n",
        "    result = subprocess.run(cmd_extract, capture_output=True, text=True)\n",
        "    if result.returncode != 0: print(f\"    Error in 3_extract_fields.sh: {result.stderr}\") #else: print(f\"    Extract Output: {result.stdout[:200]}...\")\n",
        "\n",
        "    print(f\"  [4/4] Tokenizing extracted fields to Arrow format in {FINAL_ARROW_LABEL_DIR_C}...\")\n",
        "    if not os.path.exists(EXTRACTED_FIELDS_DIR_C) or not os.listdir(EXTRACTED_FIELDS_DIR_C):\n",
        "        print(f\"    No subfolders found in {EXTRACTED_FIELDS_DIR_C} to tokenize. Check previous steps.\")\n",
        "        continue\n",
        "\n",
        "    for pcap_subfolder_name_colab in os.listdir(EXTRACTED_FIELDS_DIR_C):\n",
        "        pcap_subfolder_path_colab = os.path.join(EXTRACTED_FIELDS_DIR_C, pcap_subfolder_name_colab)\n",
        "        if os.path.isdir(pcap_subfolder_path_colab):\n",
        "            TEMP_SHARD_DIR_C = os.path.join(FINAL_ARROW_LABEL_DIR_C, f\"{pcap_subfolder_name_colab}_shards\")\n",
        "            os.makedirs(TEMP_SHARD_DIR_C, exist_ok=True)\n",
        "\n",
        "            print(f\"    Tokenizing flows from {pcap_subfolder_path_colab} (ID: '{pcap_subfolder_name_colab}')...\")\n",
        "            cmd_tokenize = [\"python3\", TOKENIZE_PY_SCRIPT_COLAB,\n",
        "                            \"--conf_file\", TOKENIZER_JSON_CONFIG_COLAB,\n",
        "                            \"--input_dir\", pcap_subfolder_path_colab,\n",
        "                            \"--output_dir\", TEMP_SHARD_DIR_C,\n",
        "                            \"--label\", pcap_subfolder_name_colab, # Store pcap_folder_name in 'labels' column\n",
        "                            \"--cores\", \"1\"]\n",
        "            result = subprocess.run(cmd_tokenize, capture_output=True, text=True)\n",
        "            if result.returncode != 0: print(f\"      Error in Tokenize.py for {pcap_subfolder_name_colab}: {result.stderr}\") #else: print(f\"      Tokenize Output: {result.stdout[:200]}...\")\n",
        "\n",
        "            COMBINED_ARROW_FILE_C = os.path.join(FINAL_ARROW_LABEL_DIR_C, f\"{pcap_subfolder_name_colab}.arrow\")\n",
        "            print(f\"    Combining shards for '{pcap_subfolder_name_colab}' into {COMBINED_ARROW_FILE_C}...\")\n",
        "            cmd_collect = [\"python3\", COLLECT_PY_SCRIPT_COLAB, TEMP_SHARD_DIR_C, COMBINED_ARROW_FILE_C]\n",
        "            result = subprocess.run(cmd_collect, capture_output=True, text=True)\n",
        "            if result.returncode != 0: print(f\"      Error in CollectTokensInFiles.py for {pcap_subfolder_name_colab}: {result.stderr}\") #else: print(f\"      Collect Output: {result.stdout[:200]}...\")\n",
        "\n",
        "            if os.path.exists(TEMP_SHARD_DIR_C):\n",
        "                shutil.rmtree(TEMP_SHARD_DIR_C)\n",
        "            print(f\"    Done with '{pcap_subfolder_name_colab}'. Arrow file at: {COMBINED_ARROW_FILE_C}\")\n",
        "\n",
        "print(\"\\n--- Colab Preprocessing Finished ---\")\n",
        "print(f\"Final Arrow files should be in: {FINAL_ARROW_OUTPUT_BASE_COLAB}/<label_dir_name>/<original_pcap_basename>.arrow\")\n",
        "print(\"The 'labels' column in these Arrow files contains the <original_pcap_basename> (e.g., 'patator-multi-cloud-attack-57497_attack').\")\n",
        "print(\"You can now download this directory or use it directly in Colab for Part 2 (Training Concept Mapping).\")\n"
      ],
      "metadata": {
        "id": "rEN0h_kh3r7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52ea932-8082-4595-bbc4-7ddcb767e158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting local-style netFound PCAP preprocessing pipeline in Colab...\n",
            "netFound Repository in Colab: /content/netFound\n",
            "Input Data Root in Colab: /content/attack_data\n",
            "Tokenizer Config: /content/netFound/configs/TestFinetuningConfig.json\n",
            "Final Arrow Output Base: /content/attack_data/final/arrow_per_pcap\n",
            "\n",
            "--- Processing Label: 0 ---\n",
            "  [1/4] Filtering PCAPs from /content/attack_data/raw/0 to /content/attack_data/filtered/0...\n",
            "  [2/4] Splitting PCAPs from /content/attack_data/filtered/0 into flows in /content/attack_data/split/0...\n",
            "  [3/4] Extracting fields from flows in /content/attack_data/split/0 to /content/attack_data/extracted/0...\n",
            "  [4/4] Tokenizing extracted fields to Arrow format in /content/attack_data/final/arrow_per_pcap/0...\n",
            "    Tokenizing flows from /content/attack_data/extracted/0/patator-multi-cloud-attack-78806 (ID: 'patator-multi-cloud-attack-78806')...\n",
            "    Combining shards for 'patator-multi-cloud-attack-78806' into /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-78806.arrow...\n",
            "    Done with 'patator-multi-cloud-attack-78806'. Arrow file at: /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-78806.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/0/patator-multi-cloud-attack-3752 (ID: 'patator-multi-cloud-attack-3752')...\n",
            "    Combining shards for 'patator-multi-cloud-attack-3752' into /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-3752.arrow...\n",
            "    Done with 'patator-multi-cloud-attack-3752'. Arrow file at: /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-3752.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/0/patator-multi-cloud-attack-37173 (ID: 'patator-multi-cloud-attack-37173')...\n",
            "    Combining shards for 'patator-multi-cloud-attack-37173' into /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-37173.arrow...\n",
            "    Done with 'patator-multi-cloud-attack-37173'. Arrow file at: /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-37173.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/0/patator-multi-cloud-attack-92898 (ID: 'patator-multi-cloud-attack-92898')...\n",
            "    Combining shards for 'patator-multi-cloud-attack-92898' into /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-92898.arrow...\n",
            "    Done with 'patator-multi-cloud-attack-92898'. Arrow file at: /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-92898.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/0/patator-multi-cloud-attack-14472 (ID: 'patator-multi-cloud-attack-14472')...\n",
            "    Combining shards for 'patator-multi-cloud-attack-14472' into /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-14472.arrow...\n",
            "    Done with 'patator-multi-cloud-attack-14472'. Arrow file at: /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-14472.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/0/patator-multi-cloud-attack-94399 (ID: 'patator-multi-cloud-attack-94399')...\n",
            "    Combining shards for 'patator-multi-cloud-attack-94399' into /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-94399.arrow...\n",
            "    Done with 'patator-multi-cloud-attack-94399'. Arrow file at: /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-94399.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/0/patator-multi-cloud-attack-57497 (ID: 'patator-multi-cloud-attack-57497')...\n",
            "    Combining shards for 'patator-multi-cloud-attack-57497' into /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-57497.arrow...\n",
            "    Done with 'patator-multi-cloud-attack-57497'. Arrow file at: /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-57497.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/0/patator-multi-cloud-attack-109371 (ID: 'patator-multi-cloud-attack-109371')...\n",
            "    Combining shards for 'patator-multi-cloud-attack-109371' into /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-109371.arrow...\n",
            "    Done with 'patator-multi-cloud-attack-109371'. Arrow file at: /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-109371.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/0/patator-multi-cloud-attack-98709 (ID: 'patator-multi-cloud-attack-98709')...\n",
            "    Combining shards for 'patator-multi-cloud-attack-98709' into /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-98709.arrow...\n",
            "    Done with 'patator-multi-cloud-attack-98709'. Arrow file at: /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-98709.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/0/patator-multi-cloud-attack-126476 (ID: 'patator-multi-cloud-attack-126476')...\n",
            "    Combining shards for 'patator-multi-cloud-attack-126476' into /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-126476.arrow...\n",
            "    Done with 'patator-multi-cloud-attack-126476'. Arrow file at: /content/attack_data/final/arrow_per_pcap/0/patator-multi-cloud-attack-126476.arrow\n",
            "\n",
            "--- Processing Label: 1 ---\n",
            "  [1/4] Filtering PCAPs from /content/attack_data/raw/1 to /content/attack_data/filtered/1...\n",
            "  [2/4] Splitting PCAPs from /content/attack_data/filtered/1 into flows in /content/attack_data/split/1...\n",
            "  [3/4] Extracting fields from flows in /content/attack_data/split/1 to /content/attack_data/extracted/1...\n",
            "  [4/4] Tokenizing extracted fields to Arrow format in /content/attack_data/final/arrow_per_pcap/1...\n",
            "    Tokenizing flows from /content/attack_data/extracted/1/patator-multi-cloud-benign2-59009 (ID: 'patator-multi-cloud-benign2-59009')...\n",
            "    Combining shards for 'patator-multi-cloud-benign2-59009' into /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-59009.arrow...\n",
            "    Done with 'patator-multi-cloud-benign2-59009'. Arrow file at: /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-59009.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/1/patator-multi-cloud-benign2-200496 (ID: 'patator-multi-cloud-benign2-200496')...\n",
            "    Combining shards for 'patator-multi-cloud-benign2-200496' into /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-200496.arrow...\n",
            "    Done with 'patator-multi-cloud-benign2-200496'. Arrow file at: /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-200496.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/1/patator-multi-cloud-benign2-228357 (ID: 'patator-multi-cloud-benign2-228357')...\n",
            "    Combining shards for 'patator-multi-cloud-benign2-228357' into /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-228357.arrow...\n",
            "    Done with 'patator-multi-cloud-benign2-228357'. Arrow file at: /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-228357.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/1/patator-multi-cloud-benign2-113166 (ID: 'patator-multi-cloud-benign2-113166')...\n",
            "    Combining shards for 'patator-multi-cloud-benign2-113166' into /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-113166.arrow...\n",
            "    Done with 'patator-multi-cloud-benign2-113166'. Arrow file at: /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-113166.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/1/patator-multi-cloud-benign2-193503 (ID: 'patator-multi-cloud-benign2-193503')...\n",
            "    Combining shards for 'patator-multi-cloud-benign2-193503' into /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-193503.arrow...\n",
            "    Done with 'patator-multi-cloud-benign2-193503'. Arrow file at: /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-193503.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/1/patator-multi-cloud-benign2-8582 (ID: 'patator-multi-cloud-benign2-8582')...\n",
            "    Combining shards for 'patator-multi-cloud-benign2-8582' into /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-8582.arrow...\n",
            "    Done with 'patator-multi-cloud-benign2-8582'. Arrow file at: /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-8582.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/1/patator-multi-cloud-benign2-0910 (ID: 'patator-multi-cloud-benign2-0910')...\n",
            "    Combining shards for 'patator-multi-cloud-benign2-0910' into /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-0910.arrow...\n",
            "    Done with 'patator-multi-cloud-benign2-0910'. Arrow file at: /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-0910.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/1/patator-multi-cloud-benign2-11603 (ID: 'patator-multi-cloud-benign2-11603')...\n",
            "    Combining shards for 'patator-multi-cloud-benign2-11603' into /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-11603.arrow...\n",
            "    Done with 'patator-multi-cloud-benign2-11603'. Arrow file at: /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-11603.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/1/patator-multi-cloud-benign2-99419 (ID: 'patator-multi-cloud-benign2-99419')...\n",
            "    Combining shards for 'patator-multi-cloud-benign2-99419' into /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-99419.arrow...\n",
            "    Done with 'patator-multi-cloud-benign2-99419'. Arrow file at: /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-99419.arrow\n",
            "    Tokenizing flows from /content/attack_data/extracted/1/patator-multi-cloud-benign2-195329 (ID: 'patator-multi-cloud-benign2-195329')...\n",
            "    Combining shards for 'patator-multi-cloud-benign2-195329' into /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-195329.arrow...\n",
            "    Done with 'patator-multi-cloud-benign2-195329'. Arrow file at: /content/attack_data/final/arrow_per_pcap/1/patator-multi-cloud-benign2-195329.arrow\n",
            "\n",
            "--- Colab Preprocessing Finished ---\n",
            "Final Arrow files should be in: /content/attack_data/final/arrow_per_pcap/<label_dir_name>/<original_pcap_basename>.arrow\n",
            "The 'labels' column in these Arrow files contains the <original_pcap_basename> (e.g., 'patator-multi-cloud-attack-57497_attack').\n",
            "You can now download this directory or use it directly in Colab for Part 2 (Training Concept Mapping).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 1 (Part 2): Setup for Training - Uploads & Path Verification (Revised for datasets) ---\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "print(\"--- Pip Installing/Updating Packages ---\")\n",
        "# Install specific versions to ensure consistency and known good states\n",
        "# Using pip install --upgrade to ensure we get the specified version or newer if compatible\n",
        "# Using -q for quieter output\n",
        "# subprocess.run([\"pip\", \"install\", \"-q\", \"-U\",\n",
        "#                 \"transformers==4.38.2\",  # A recent stable version\n",
        "#                 \"datasets==2.18.0\",    # A recent stable version, known to have from_arrow\n",
        "#                 \"torch\",\n",
        "#                 \"numpy\",\n",
        "#                 \"scikit-learn\",\n",
        "#                 \"huggingface_hub\",\n",
        "#                 \"pyarrow\"], check=True)\n",
        "print(\"Python packages (transformers, datasets, torch, etc.) ensured.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fDLvgaRpV8y",
        "outputId": "ba75d2ee-7e3c-4ac0-b47f-ec5bc9951c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Pip Installing/Updating Packages ---\n",
            "Python packages (transformers, datasets, torch, etc.) ensured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- User Action Required: Upload Files (if not already present) ---\")\n",
        "# (Keep your existing upload instructions here)\n",
        "print(\"1. Your 'concepts.txt' file to /content/concepts.txt.\")\n",
        "print(\"2. Your 'concept_scores.txt' file to /content/concept_scores.txt.\")\n",
        "print(\"3. The 'arrow_per_pcap/' directory to /content/arrow_per_pcap/ (containing 0/*.arrow and 1/*.arrow files).\")\n",
        "print(\"4. The 'src/train/' directory from your 'netFound' clone to /content/netFound/src/train/.\")\n",
        "print(\"5. Your 'config.json' to /content/config.json\")\n",
        "\n",
        "# Verify essential uploaded files/directories (paths based on your previous script)\n",
        "CONCEPTS_FILE_PATH_PT2 = \"/content/concepts.txt\"\n",
        "BINNED_CONCEPT_SCORES_FILE_PATH_PT2 = \"/content/concept_scores.txt\"\n",
        "ARROW_DATA_BASE_DIR_PT2 = \"/content/attack_data/final/arrow_per_pcap\"\n",
        "NETFOUND_TRAIN_SRC_DIR_PT2 = \"/content/netFound/netFound/src/train\" # Adjusted as per your last path\n",
        "UPLOADED_CONFIG_JSON_PATH_PT2 = \"/content/config.json\"\n",
        "\n",
        "# Basic checks\n",
        "if not os.path.exists(CONCEPTS_FILE_PATH_PT2): print(f\"ERROR: '{CONCEPTS_FILE_PATH_PT2}' not found.\")\n",
        "if not os.path.exists(BINNED_CONCEPT_SCORES_FILE_PATH_PT2): print(f\"ERROR: '{BINNED_CONCEPT_SCORES_FILE_PATH_PT2}' not found.\")\n",
        "if not os.path.isdir(ARROW_DATA_BASE_DIR_PT2): print(f\"ERROR: Arrow data directory '{ARROW_DATA_BASE_DIR_PT2}' not found.\")\n",
        "if not os.path.isdir(NETFOUND_TRAIN_SRC_DIR_PT2): print(f\"ERROR: netFound src/train directory '{NETFOUND_TRAIN_SRC_DIR_PT2}' not found.\")\n",
        "if not os.path.exists(UPLOADED_CONFIG_JSON_PATH_PT2): print(f\"ERROR: '{UPLOADED_CONFIG_JSON_PATH_PT2}' not found.\")\n",
        "\n",
        "# Add to Python path\n",
        "netfound_src_dir = os.path.dirname(NETFOUND_TRAIN_SRC_DIR_PT2) # /content/netFound/netFound/src\n",
        "if NETFOUND_TRAIN_SRC_DIR_PT2 not in sys.path: sys.path.insert(0, NETFOUND_TRAIN_SRC_DIR_PT2)\n",
        "if netfound_src_dir not in sys.path: sys.path.insert(0, netfound_src_dir)\n",
        "\n",
        "print(\"\\nSetup check complete. If no ERRORs, proceed to the next cell for training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqgQk3mbGAVR",
        "outputId": "0cba6d46-0b96-42ca-943e-b59219f8f802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- User Action Required: Upload Files (if not already present) ---\n",
            "1. Your 'concepts.txt' file to /content/concepts.txt.\n",
            "2. Your 'concept_scores.txt' file to /content/concept_scores.txt.\n",
            "3. The 'arrow_per_pcap/' directory to /content/arrow_per_pcap/ (containing 0/*.arrow and 1/*.arrow files).\n",
            "4. The 'src/train/' directory from your 'netFound' clone to /content/netFound/src/train/.\n",
            "5. Your 'config.json' to /content/config.json\n",
            "ERROR: '/content/config.json' not found.\n",
            "\n",
            "Setup check complete. If no ERRORs, proceed to the next cell for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet datasets\n"
      ],
      "metadata": {
        "id": "Exdv249GIsm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed6c7f4-152c-467a-a371-005480efe977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 2 (Part 2): Training the Concept Mapping Model (Robust Arrow Reading + Pandas Fallback) ---\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "import pyarrow as pa\n",
        "import pyarrow.ipc as ipc\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "from datasets import Dataset as HFDataset, concatenate_datasets\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# 1) Patch sys.path to include netFound source\n",
        "TRAIN_SRC = \"/content/netFound/netFound/src/train\"\n",
        "sys.path.insert(0, TRAIN_SRC)\n",
        "from NetFoundModels    import NetFoundBase, AttentivePooling\n",
        "from NetfoundConfig    import NetfoundConfig\n",
        "from NetfoundTokenizer import NetFoundTokenizer\n",
        "\n",
        "# 2) Configuration\n",
        "HF_MODEL            = \"snlucsb/netFound-640M-base\"\n",
        "CONCEPTS_FILE       = \"/content/concepts.txt\"\n",
        "SCORES_FILE         = \"/content/concept_scores.txt\"\n",
        "CONFIG_JSON         = \"/content/config.json\"\n",
        "ARROW_DATA_DIR      = \"/content/attack_data/final/arrow_per_pcap\"\n",
        "ID_FIELD            = \"labels\"\n",
        "BATCH_SIZE          = 4\n",
        "LR                  = 5e-5\n",
        "EPOCHS              = 25\n",
        "DEVICE              = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 3) Load base concepts\n",
        "with open(CONCEPTS_FILE, 'r', encoding='utf-8') as f:\n",
        "    base_concepts = [l.strip() for l in f if l.strip()]\n",
        "concept2idx = {c:i for i,c in enumerate(base_concepts)}\n",
        "NUM_CONCEPTS = len(base_concepts)\n",
        "\n",
        "# 4) Parse binned scores\n",
        "binned_scores = {}\n",
        "cur_id, buf = None, []\n",
        "with open(SCORES_FILE, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        m = re.match(r\"File:\\s*packet_(.+?)\\.txt\", line)\n",
        "        if m:\n",
        "            if cur_id and buf:\n",
        "                t = torch.zeros(NUM_CONCEPTS, dtype=torch.long)\n",
        "                for cname, b in buf:\n",
        "                    t[concept2idx[cname]] = b\n",
        "                binned_scores[cur_id] = t\n",
        "            cur_id, buf = m.group(1), []\n",
        "        else:\n",
        "            m2 = re.match(r'-\\s*\"(.*?)\".*:\\s*(\\d+)', line)\n",
        "            if m2 and cur_id:\n",
        "                buf.append((m2.group(1).strip(), int(m2.group(2))))\n",
        "    if cur_id and buf:\n",
        "        t = torch.zeros(NUM_CONCEPTS, dtype=torch.long)\n",
        "        for cname, b in buf:\n",
        "            t[concept2idx[cname]] = b\n",
        "        binned_scores[cur_id] = t\n",
        "assert binned_scores, \"No binned scores parsed!\"\n",
        "\n",
        "# 5) Load NetfoundConfig\n",
        "with open(CONFIG_JSON, 'r') as f:\n",
        "    cfgd = json.load(f)\n",
        "defaults = {\n",
        "    \"vocab_size\":65539, \"hidden_size\":1024, \"num_hidden_layers\":24,\n",
        "    \"num_attention_heads\":16, \"intermediate_size\":3072,\n",
        "    \"max_bursts\":12, \"max_burst_length\":109,\n",
        "    \"metaFeatures\":4, \"pad_token_id\": NetFoundTokenizer.PAD_TOKEN\n",
        "}\n",
        "for k,v in defaults.items():\n",
        "    cfgd.setdefault(k, v)\n",
        "if \"model_max_length\" not in cfgd:\n",
        "    cfgd[\"model_max_length\"] = cfgd[\"max_bursts\"] * cfgd[\"max_burst_length\"]\n",
        "net_cfg = NetfoundConfig(**cfgd)\n",
        "\n",
        "# 6) Instantiate & freeze NetFoundBase\n",
        "net_base = NetFoundBase(config=net_cfg)\n",
        "try:\n",
        "    path = hf_hub_download(repo_id=HF_MODEL, filename=\"pytorch_model.bin\")\n",
        "    sd   = torch.load(path, map_location=\"cpu\")\n",
        "    adapted = {k.replace(\"base_transformer.\",\"\"):v for k,v in sd.items() if k.startswith(\"base_transformer.\")}\n",
        "    if not adapted:\n",
        "        adapted = {k:v for k,v in sd.items() if k in net_base.state_dict()}\n",
        "    net_base.load_state_dict(adapted, strict=False)\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è HF weights not loaded, using random init.\")\n",
        "for p in net_base.parameters(): p.requires_grad = False\n",
        "net_base.to(DEVICE).eval()\n",
        "\n",
        "pooler = AttentivePooling(config=net_cfg).to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "class ConceptMapper(nn.Module):\n",
        "    def __init__(self, hid, n_concepts, n_bins):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hid, hid//2), nn.ReLU(), nn.Dropout(0.25)\n",
        "        )\n",
        "        self.heads = nn.ModuleList([nn.Linear(hid//2, n_bins) for _ in range(n_concepts)])\n",
        "    def forward(self, x):\n",
        "        h = self.fc(x)\n",
        "        return torch.stack([hd(h) for hd in self.heads], dim=1)\n",
        "\n",
        "NUM_BINS = max(t.max().item() for t in binned_scores.values()) + 1\n",
        "mapper   = ConceptMapper(net_cfg.hidden_size, NUM_CONCEPTS, NUM_BINS).to(DEVICE)\n",
        "# Helper (must be defined before __init__)\n",
        "# 1) Robust Arrow loader (file‚ÄêIPC, then stream‚ÄêIPC)\n",
        "def _load_arrow_table(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        try:\n",
        "            return ipc.open_file(f).read_all()\n",
        "        except pa.lib.ArrowInvalid:\n",
        "            f.seek(0)\n",
        "            return ipc.open_stream(f).read_all()\n",
        "\n",
        "class ConceptDataset(TorchDataset):\n",
        "    def __init__(self, arrow_dir, score_map, cfg):\n",
        "        \"\"\"\n",
        "        arrow_dir: path to .../arrow_per_pcap\n",
        "        score_map: dict mapping pcap_id (with suffix) -> binned-score Tensor\n",
        "        cfg:       NetfoundConfig\n",
        "        \"\"\"\n",
        "        self.score_map = score_map\n",
        "        self.cfg       = cfg\n",
        "        self.mb        = cfg.max_bursts\n",
        "        self.mbl       = cfg.max_burst_length\n",
        "\n",
        "        # 2) Gather only those .arrow files whose pcap_id is in score_map\n",
        "        self.entries = []\n",
        "        for lbl in [\"0\", \"1\"]:\n",
        "            sub = os.path.join(arrow_dir, lbl)\n",
        "            if not os.path.isdir(sub):\n",
        "                continue\n",
        "            for fn in os.listdir(sub):\n",
        "                if not fn.endswith(\".arrow\"):\n",
        "                    continue\n",
        "                base = os.path.splitext(fn)[0]                # e.g. \"patator-multi-cloud-attack-57497\"\n",
        "                # detect suffix in filename: it might already include \"_attack\" or \"_benign2\"\n",
        "                # so if base endswith \"_attack\" or \"_benign2\", use it directly; else append the folder-based suffix\n",
        "                if base.endswith(\"_attack\") or base.endswith(\"_benign2\"):\n",
        "                    pid = base\n",
        "                else:\n",
        "                    suffix = \"attack\" if lbl==\"0\" else \"benign2\"\n",
        "                    pid = f\"{base}_{suffix}\"\n",
        "\n",
        "                if pid in score_map:\n",
        "                    fullpath = os.path.join(sub, fn)\n",
        "                    self.entries.append((fullpath, pid))\n",
        "\n",
        "        assert self.entries, \"No .arrow files matched your concept_scores keys!\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, pid = self.entries[idx]\n",
        "        # 3) Load the entire Arrow table for this pcap\n",
        "        table = _load_arrow_table(path)\n",
        "        data  = table.to_pydict()  # dict of lists, e.g. data[\"burst_tokens\"]\n",
        "\n",
        "        # 4) Build input_ids & attention_mask\n",
        "        cls, pad = NetFoundTokenizer.CLS_TOKEN, NetFoundTokenizer.PAD_TOKEN\n",
        "        toks, mask = [], []\n",
        "        bursts = data[\"burst_tokens\"]\n",
        "        for b in range(self.mb):\n",
        "            seq = [cls] + list(bursts[b]) if b < len(bursts) else []\n",
        "            seq = seq[:self.mbl] + [pad] * (self.mbl - len(seq))\n",
        "            toks.extend(seq)\n",
        "            L = min(len(bursts[b]) + 1, self.mbl) if b < len(bursts) else 0\n",
        "            mask.extend([1] * L + [0] * (self.mbl - L))\n",
        "\n",
        "        inp = {\n",
        "            \"input_ids\":      torch.tensor(toks, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "        # 5) Attach meta-features\n",
        "        for mk, arrk in zip(\n",
        "            [\"directions\", \"iats\", \"bytes\", \"pkt_count\"],\n",
        "            [\"directions\",  \"iats\",  \"bytes\",  \"counts\"]\n",
        "        ):\n",
        "            vals, feat = data[arrk], []\n",
        "            for b in range(self.mb):\n",
        "                v = float(vals[b]) if b < len(vals) else 0.0\n",
        "                if mk == \"directions\" and b < len(vals):\n",
        "                    v = 1.0 if vals[b] else -1.0\n",
        "                for i in range(b * self.mbl, (b + 1) * self.mbl):\n",
        "                    feat.append(v if mask[i] else 0.0)\n",
        "            inp[mk] = torch.tensor(feat, dtype=torch.float)\n",
        "\n",
        "        # 6) Protocol feature\n",
        "        proto_vals = data[\"protocol\"]\n",
        "        proto_id   = proto_vals[0] if len(proto_vals)>0 else 0\n",
        "        inp[\"protocol\"] = torch.tensor(proto_id, dtype=torch.long)\n",
        "\n",
        "        # 7) Return inputs + target vector\n",
        "        target = self.score_map[pid]  # torch.LongTensor of shape [num_concepts]\n",
        "        return inp, target\n",
        "\n",
        "# --- Usage: ---\n",
        "dataset = ConceptDataset(ARROW_DATA_DIR, binned_scores, net_cfg)\n",
        "print(\"Num samples:\", len(dataset))  # should now > 0\n",
        "loader  = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "# 8) Training Loop\n",
        "# 8) Training Loop\n",
        "opt  = optim.AdamW(mapper.parameters(), lr=LR)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    mapper.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        # Move everything to device\n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "\n",
        "        # Prepare NetFound inputs\n",
        "        model_inputs = {\n",
        "            \"input_ids\":      inputs[\"input_ids\"],\n",
        "            \"attention_mask\": inputs[\"attention_mask\"],\n",
        "            \"protocol\":       inputs[\"protocol\"],   # shape [batch]\n",
        "            \"direction\":      inputs[\"directions\"],\n",
        "            \"iats\":           inputs[\"iats\"],\n",
        "            \"bytes\":          inputs[\"bytes\"],\n",
        "            \"pkt_count\":      inputs[\"pkt_count\"],\n",
        "            \"return_dict\":    True,                # ensure ModelOutput is returned\n",
        "        }\n",
        "\n",
        "        # 1) Forward through NetFoundBase\n",
        "        with torch.no_grad():\n",
        "            base_out = net_base(**model_inputs)\n",
        "            hidden   = base_out.last_hidden_state   # [batch, seq_len, hidden_size]\n",
        "\n",
        "        # 2) Pool & map\n",
        "        reps   = hidden[:, ::net_cfg.max_burst_length, :]  # [batch, bursts, hidden]\n",
        "        h_tok  = pooler(reps)                              # [batch, hidden]\n",
        "        logits = mapper(h_tok)                             # [batch, concepts, bins]\n",
        "\n",
        "        # 3) Compute loss & update\n",
        "        loss = sum(crit(logits[:, c, :], targets[:, c])\n",
        "                   for c in range(NUM_CONCEPTS))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # 4) Log\n",
        "    avg_loss = total_loss / (len(loader) * NUM_CONCEPTS)\n",
        "    print(f\"Epoch {ep}/{EPOCHS} ‚Äî Avg loss/concept: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"‚úÖ Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-zLe6cvqRLD",
        "outputId": "e7a7881b-c7c7-4cb9-f54c-0dae9b01c698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num samples: 10\n",
            "Epoch 1/25 ‚Äî Avg loss/concept: 2.3139\n",
            "Epoch 2/25 ‚Äî Avg loss/concept: 2.2777\n",
            "Epoch 3/25 ‚Äî Avg loss/concept: 2.2547\n",
            "Epoch 4/25 ‚Äî Avg loss/concept: 2.2395\n",
            "Epoch 5/25 ‚Äî Avg loss/concept: 2.2023\n",
            "Epoch 6/25 ‚Äî Avg loss/concept: 2.1830\n",
            "Epoch 7/25 ‚Äî Avg loss/concept: 2.1443\n",
            "Epoch 8/25 ‚Äî Avg loss/concept: 2.1152\n",
            "Epoch 9/25 ‚Äî Avg loss/concept: 2.0874\n",
            "Epoch 10/25 ‚Äî Avg loss/concept: 2.0662\n",
            "Epoch 11/25 ‚Äî Avg loss/concept: 2.0317\n",
            "Epoch 12/25 ‚Äî Avg loss/concept: 2.0149\n",
            "Epoch 13/25 ‚Äî Avg loss/concept: 1.9774\n",
            "Epoch 14/25 ‚Äî Avg loss/concept: 1.9446\n",
            "Epoch 15/25 ‚Äî Avg loss/concept: 1.9232\n",
            "Epoch 16/25 ‚Äî Avg loss/concept: 1.8916\n",
            "Epoch 17/25 ‚Äî Avg loss/concept: 1.8347\n",
            "Epoch 18/25 ‚Äî Avg loss/concept: 1.8003\n",
            "Epoch 19/25 ‚Äî Avg loss/concept: 1.7902\n",
            "Epoch 20/25 ‚Äî Avg loss/concept: 1.7640\n",
            "Epoch 21/25 ‚Äî Avg loss/concept: 1.7086\n",
            "Epoch 22/25 ‚Äî Avg loss/concept: 1.6915\n",
            "Epoch 23/25 ‚Äî Avg loss/concept: 1.6526\n",
            "Epoch 24/25 ‚Äî Avg loss/concept: 1.6132\n",
            "Epoch 25/25 ‚Äî Avg loss/concept: 1.5909\n",
            "‚úÖ Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Freeze your trained ConceptMapper\n",
        "for p in mapper.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Define the output head (concepts -> controller label)\n",
        "NUM_OUTPUT_CLASSES = 2  # set to your number of classes\n",
        "output_head = nn.Linear(NUM_CONCEPTS, NUM_OUTPUT_CLASSES).to(DEVICE)\n",
        "\n",
        "# Optimizer and loss\n",
        "opt2  = optim.AdamW(output_head.parameters(), lr=1e-3)\n",
        "crit2 = nn.CrossEntropyLoss()\n",
        "\n",
        "# DataLoader for controller labels (expects (inputs, y_true) tuples)\n",
        "# e.g., controller_loader = DataLoader(controller_dataset, ...)\n",
        "\n",
        "NUM_EPOCHS_OUT = 10\n",
        "for epoch in range(1, NUM_EPOCHS_OUT + 1):\n",
        "    output_head.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, y_true in controller_loader:\n",
        "        # move to device\n",
        "        for k,v in inputs.items(): inputs[k] = v.to(DEVICE)\n",
        "        y_true = y_true.to(DEVICE)\n",
        "\n",
        "        # 1) Get frozen embeddings -> concept features\n",
        "        with torch.no_grad():\n",
        "            base_out = net_base(\n",
        "                input_ids=inputs['input_ids'],\n",
        "                attention_mask=inputs['attention_mask'],\n",
        "                protocol=inputs['protocol'],\n",
        "                direction=inputs['directions'],\n",
        "                iats=inputs['iats'],\n",
        "                bytes=inputs['bytes'],\n",
        "                pkt_count=inputs['pkt_count'],\n",
        "                return_dict=True\n",
        "            )\n",
        "            hidden = base_out.last_hidden_state\n",
        "            reps   = hidden[:, ::net_cfg.max_burst_length, :]\n",
        "            h_tok  = pooler(reps)\n",
        "            logits_c = mapper(h_tok)  # [B, C, bins]\n",
        "            concept_feats = logits_c.argmax(dim=-1).float()  # [B, C]\n",
        "\n",
        "        # 2) Forward through output head\n",
        "        y_hat = output_head(concept_feats)\n",
        "        loss  = crit2(y_hat, y_true)\n",
        "\n",
        "        # 3) Backprop on output head only\n",
        "        opt2.zero_grad()\n",
        "        loss.backward()\n",
        "        opt2.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg = total_loss / len(controller_loader)\n",
        "    print(f\"[Output Head] Epoch {epoch}/{NUM_EPOCHS_OUT} - Avg Loss: {avg:.4f}\")\n"
      ],
      "metadata": {
        "id": "WlKoVsJcvmso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSGLacm24PQt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}